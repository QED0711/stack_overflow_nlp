{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_pickle('../data/interum/text_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111102</td>\n",
       "      <td>&lt;p&gt;How would you explain JavaScript closures t...</td>\n",
       "      <td>javascript|scope|closures</td>\n",
       "      <td>\\n\\nHow would you explain JavaScript closures ...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>would explain closure someone knowledge concep...</td>\n",
       "      <td>[would, explain, closure, someone, knowledge, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1642028</td>\n",
       "      <td>&lt;p&gt;After reading &lt;a href= http://groups.google...</td>\n",
       "      <td>c++|c|operators|code-formatting|standards-comp...</td>\n",
       "      <td>\\n\\nAfter reading Hidden Features and Dark Cor...</td>\n",
       "      <td>c++</td>\n",
       "      <td>read hidden feature dark corner c stl comp lan...</td>\n",
       "      <td>[read, hidden, feature, dark, corner, c, stl, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               body  \\\n",
       "0   111102  <p>How would you explain JavaScript closures t...   \n",
       "1  1642028  <p>After reading <a href= http://groups.google...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                          javascript|scope|closures   \n",
       "1  c++|c|operators|code-formatting|standards-comp...   \n",
       "\n",
       "                                                text      target  \\\n",
       "0  \\n\\nHow would you explain JavaScript closures ...  javascript   \n",
       "1  \\n\\nAfter reading Hidden Features and Dark Cor...         c++   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  would explain closure someone knowledge concep...   \n",
       "1  read hidden feature dark corner c stl comp lan...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [would, explain, closure, someone, knowledge, ...  \n",
       "1  [read, hidden, feature, dark, corner, c, stl, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into features and target \n",
    "feature = posts_df['cleaned_text']\n",
    "label = posts_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    feature, label, stratify = label, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidVector Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24241x57072 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 949219 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try tfidVector \n",
    "tfid_vect = TfidfVectorizer()\n",
    "tfid_vect.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training and test sets \n",
    "x_train_tfidf = tfid_vect.transform(train_X)\n",
    "x_test_tfidf =tfid_vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CountVectorizer.get_feature_names of TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)>\n"
     ]
    }
   ],
   "source": [
    "print(tfid_vect.get_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initlaize all models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "lg = LogisticRegression(random_state=42)\n",
    "svc = SVC(random_state=42)\n",
    "nb = MultinomialNB()\n",
    "# xg = XGBClassifier(random_state = 42)\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "test: [0.74285361 0.74569979 0.74811239]\n",
      "train: [0.9977104  0.99740099 0.99733944]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "test: [0.80076723 0.79940601 0.79848991]\n",
      "train: [0.88768564 0.88935644 0.89419626]\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=42,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "test: [0.72899394 0.74186363 0.70441886]\n",
      "train: [0.77475248 0.7927599  0.74433857]\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "test: [0.75559955 0.76339562 0.76667904]\n",
      "train: [0.84269802 0.84263614 0.84333622]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "test: [0.72565277 0.72701398 0.7392004 ]\n",
      "train: [0.74690594 0.74684406 0.74248237]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "test: [0.74186363 0.74421482 0.75628172]\n",
      "train: [0.78842822 0.78644802 0.78474199]\n"
     ]
    }
   ],
   "source": [
    "# try vanilla models \n",
    "for base_clf in (rf,lg,svc,nb,ada,gb):\n",
    "        clf = OneVsRestClassifier(base_clf)\n",
    "        cv_results = cross_validate(clf,x_train_tfidf,train_y, cv = 3,n_jobs=-1,return_train_score= True)\n",
    "        print(base_clf)\n",
    "        print('test:', cv_results['test_score'])\n",
    "        print('train:', cv_results['train_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will focus on:\n",
    "\n",
    "* MultinomialNB\n",
    "* RandomForest\n",
    "* LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter for logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf',lg ),\n",
    "])\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "C =uniform(loc=0, scale=4)\n",
    "max_df = np.linspace(0.3,0.75,num = 10)\n",
    "min_df = np.arange(1,16,2)\n",
    "max_features = np.arange(5000,10000,500)\n",
    "parameters = {\n",
    "        'vect__ngram_range':((1,1),(1,2)),\n",
    "        'vect__max_df': max_df,\n",
    "        'vect__min_df':min_df,\n",
    "        'vect__max_features':max_features,\n",
    "        'clf': (lg,),\n",
    "        'clf__penalty': ('l1','l2'),\n",
    "        'clf__C': C}\n",
    "clf_lg = RandomizedSearchCV(pipeline, parameters,random_state = 42, n_iter=100, cv = 3, n_jobs=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=Pipeline(memory=None,\n",
       "                                      steps=[('vect',\n",
       "                                              TfidfVectorizer(analyzer='word',\n",
       "                                                              binary=False,\n",
       "                                                              decode_error='strict',\n",
       "                                                              dtype=<class 'numpy.float64'>,\n",
       "                                                              encoding='utf-8',\n",
       "                                                              input='content',\n",
       "                                                              lowercase=True,\n",
       "                                                              max_df=1.0,\n",
       "                                                              max_features=None,\n",
       "                                                              min_df=1,\n",
       "                                                              ngram_range=(1,\n",
       "                                                                           1),\n",
       "                                                              norm='l2',\n",
       "                                                              preprocessor=None,\n",
       "                                                              smooth_idf=True,\n",
       "                                                              stop...\n",
       "                                        'clf__penalty': ('l1', 'l2'),\n",
       "                                        'vect__max_df': array([0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75]),\n",
       "                                        'vect__max_features': array([5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500]),\n",
       "                                        'vect__min_df': array([ 1,  3,  5,  7,  9, 11, 13, 15]),\n",
       "                                        'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lg.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(C=2.343102325093853, class_weight=None, dual=False,\n",
       "                    fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
       "                    max_iter=100, multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'clf__C': 2.343102325093853,\n",
       " 'clf__penalty': 'l1',\n",
       " 'vect__max_df': 0.45,\n",
       " 'vect__max_features': 8500,\n",
       " 'vect__min_df': 7,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8127965017944804"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_lg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finer search\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', lg),\n",
    "])\n",
    "parameters = {\n",
    "        'vect__ngram_range':((1,2),),\n",
    "        'vect__max_df': (0.4,0.5),\n",
    "        'vect__min_df':(6,8),\n",
    "        'vect__max_features':(8000,9000),\n",
    "        'clf': (lg,),\n",
    "        'clf__penalty': ('l1','l2'),\n",
    "        'clf__C': (2,3,4)}\n",
    "grid_search_lg = GridSearchCV(pipeline, parameters, cv = 3, return_train_score= True,n_jobs=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                    multi_class='warn',\n",
       "                                                    n_jobs=None, penalty='l1',\n",
       "                                                    random_state=42,\n",
       "                                                    solver='warn', tol=0.0001,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),),\n",
       "                         'clf__C': (2, 3, 4), 'clf__penalty': ('l1', 'l2'),\n",
       "                         'vect__max_df': (0.4, 0.5),\n",
       "                         'vect__max_features': (8000, 9000),\n",
       "                         'vect__min_df': (6, 8),\n",
       "                         'vect__ngram_range': ((1, 2),)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_lg.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(C=2, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'clf__C': 2,\n",
       " 'clf__penalty': 'l1',\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__max_features': 9000,\n",
       " 'vect__min_df': 6,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_lg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8929912132337775"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_lg.score(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8201616894901831"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_lg.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = grid_search_lg.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = grid_search_lg.best_estimator_['clf'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 9000)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurenames = grid_search_lg.best_estimator_['vect'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict={}\n",
    "for i, cls in enumerate(classes):\n",
    "    coef_dict[cls]=[]\n",
    "    for c, f in zip(coefs[i],featurenames):\n",
    "        if c:\n",
    "            coef_dict[cls].append((f,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('writeline', 34.35443542592411),\n",
       " ('net', 14.409952259671503),\n",
       " ('msdn', 13.406405098449603),\n",
       " ('ienumerable', 12.597638498674106),\n",
       " ('script jquery', 12.258235070200952),\n",
       " ('streamreader', 12.207305885657282),\n",
       " ('linq', 12.060271676062444),\n",
       " ('window form', 11.696983129235713),\n",
       " ('winforms', 11.34889725246229),\n",
       " ('entity framework', 11.139816376850316)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(coef_dict['c#'],key=lambda x: x[1],reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('println', -29.522974297537914),\n",
       " ('django', -17.091895906609306),\n",
       " ('pythonic', -16.322255328863903),\n",
       " ('spring', -16.15729795551023),\n",
       " ('sun', -15.73658807129685),\n",
       " ('def', -15.077989494940644),\n",
       " ('jquery', -15.06177845407277),\n",
       " ('std', -13.810242160767265),\n",
       " ('gcc', -13.09393849381091),\n",
       " ('jvm', -12.65241004520841)]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(coef_dict['c#'],key=lambda x: x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparmater for RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf',rf),\n",
    "])\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=800, num=10)]\n",
    "# Number of features to consider at every split\n",
    "mf = ['auto', 'sqrt']\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [10, 20, 30]\n",
    "max_df = np.linspace(0.3,0.75,num = 10)\n",
    "min_df = np.arange(1,16,2)\n",
    "max_features = np.arange(5000,10000,500)\n",
    "parameters = {\n",
    "        'vect__ngram_range':((1,1),(1,2)),\n",
    "        'vect__max_df': max_df,\n",
    "        'vect__min_df':min_df,\n",
    "        'vect__max_features':max_features,\n",
    "        'clf': (rf,),\n",
    "        'clf__n_estimators': n_estimators,\n",
    "        'clf__max_features': mf,\n",
    "        'clf__min_samples_split': min_samples_split}\n",
    "clf_rf = RandomizedSearchCV(pipeline, parameters,random_state = 42, n_iter=100, cv = 3, n_jobs=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_rf.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200, 'min_samples_split': 10, 'max_features': 'auto'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multinominalNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "                   estimator=MultinomialNB(alpha=1.0, class_prior=None,\n",
       "                                           fit_prior=True),\n",
       "                   iid='warn', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'alpha': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f177db7f4e0>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create regularization penalty space\n",
    "\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "alpha =uniform(loc=0, scale=4)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha = alpha)\n",
    "clf = RandomizedSearchCV(nb, hyperparameters, random_state= 42,\n",
    "                         n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "clf.fit(x_train_tfidf,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.08233797718320979}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best tfidVector using naivebaye model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', lg),\n",
    "])\n",
    "parameters = {\n",
    "        'vect__ngram_range':((1,1),(1,2)),\n",
    "        'vect__max_df': (0.3,0.5, 0.75),\n",
    "        'vect__min_df':(5, 10,15),\n",
    "        'vect__max_features':(4000,5000,6000),\n",
    "        'clf': (lg,),\n",
    "        'clf__penalty': ('l1',),\n",
    "        'clf__C': (3.895,)}\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv = 3, return_train_score= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                    multi_class='warn',\n",
       "                                                    n_jobs=None, penalty='l1',\n",
       "                                                    random_state=42,\n",
       "                                                    solver='warn', tol=0.0001,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),),\n",
       "                         'clf__C': (3.895,), 'clf__penalty': ('l1',),\n",
       "                         'vect__max_df': (0.3, 0.5, 0.75),\n",
       "                         'vect__max_features': (4000, 5000, 6000),\n",
       "                         'vect__min_df': (5, 10, 15),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2))},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf': LogisticRegression(C=3.895, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l1',\n",
       "                    random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'clf__C': 3.895,\n",
       " 'clf__penalty': 'l1',\n",
       " 'vect__max_df': 0.5,\n",
       " 'vect__max_features': 6000,\n",
       " 'vect__min_df': 5,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8062786188688585"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24241, 57072)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
