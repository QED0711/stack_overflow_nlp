{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_pickle('../data/interum/text_target.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111102</td>\n",
       "      <td>&lt;p&gt;How would you explain JavaScript closures t...</td>\n",
       "      <td>javascript|scope|closures</td>\n",
       "      <td>\\n\\nHow would you explain JavaScript closures ...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>would explain closure someone knowledge concep...</td>\n",
       "      <td>[would, explain, closure, someone, knowledge, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1642028</td>\n",
       "      <td>&lt;p&gt;After reading &lt;a href= http://groups.google...</td>\n",
       "      <td>c++|c|operators|code-formatting|standards-comp...</td>\n",
       "      <td>\\n\\nAfter reading Hidden Features and Dark Cor...</td>\n",
       "      <td>c++</td>\n",
       "      <td>read hidden feature dark corner c stl comp lan...</td>\n",
       "      <td>[read, hidden, feature, dark, corner, c, stl, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               body  \\\n",
       "0   111102  <p>How would you explain JavaScript closures t...   \n",
       "1  1642028  <p>After reading <a href= http://groups.google...   \n",
       "\n",
       "                                                tags  \\\n",
       "0                          javascript|scope|closures   \n",
       "1  c++|c|operators|code-formatting|standards-comp...   \n",
       "\n",
       "                                                text      target  \\\n",
       "0  \\n\\nHow would you explain JavaScript closures ...  javascript   \n",
       "1  \\n\\nAfter reading Hidden Features and Dark Cor...         c++   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  would explain closure someone knowledge concep...   \n",
       "1  read hidden feature dark corner c stl comp lan...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [would, explain, closure, someone, knowledge, ...  \n",
       "1  [read, hidden, feature, dark, corner, c, stl, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into features and target \n",
    "feature = posts_df['cleaned_text']\n",
    "label = posts_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test\n",
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    feature, label, stratify = label, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try CountVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try CountVector \n",
    "count_vect = CountVectorizer()\n",
    "count_vect.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training and test sets \n",
    "x_train_count = count_vect.transform(train_X)\n",
    "x_test_count = count_vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try tfidVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24241x57072 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 949219 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try tfidVector \n",
    "tfid_vect = TfidfVectorizer()\n",
    "tfid_vect.fit_transform(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training and test sets \n",
    "x_train_tfidf = tfid_vect.transform(train_X)\n",
    "x_test_tfidf =tfid_vect.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method CountVectorizer.get_feature_names of TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None)>\n"
     ]
    }
   ],
   "source": [
    "print(tfid_vect.get_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initlaize all models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "lg = LogisticRegression(random_state=42)\n",
    "svc = SVC(random_state=42)\n",
    "nb = MultinomialNB()\n",
    "# xg = XGBClassifier(random_state = 42)\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
      "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
      "                       warm_start=False)\n",
      "test: [0.74285361 0.74569979 0.74811239]\n",
      "train: [0.9977104  0.99740099 0.99733944]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "test: [0.80076723 0.79940601 0.79848991]\n",
      "train: [0.88768564 0.88935644 0.89419626]\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "    kernel='rbf', max_iter=-1, probability=False, random_state=42,\n",
      "    shrinking=True, tol=0.001, verbose=False)\n",
      "test: [0.72899394 0.74186363 0.70441886]\n",
      "train: [0.77475248 0.7927599  0.74433857]\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "test: [0.75559955 0.76339562 0.76667904]\n",
      "train: [0.84269802 0.84263614 0.84333622]\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=42)\n",
      "test: [0.72565277 0.72701398 0.7392004 ]\n",
      "train: [0.74690594 0.74684406 0.74248237]\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "                           max_features=None, max_leaf_nodes=None,\n",
      "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                           min_samples_leaf=1, min_samples_split=2,\n",
      "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                           n_iter_no_change=None, presort='auto',\n",
      "                           random_state=42, subsample=1.0, tol=0.0001,\n",
      "                           validation_fraction=0.1, verbose=0,\n",
      "                           warm_start=False)\n",
      "test: [0.74186363 0.74421482 0.75628172]\n",
      "train: [0.78842822 0.78644802 0.78474199]\n"
     ]
    }
   ],
   "source": [
    "# try vanilla models \n",
    "for base_clf in (rf,lg,svc,nb,ada,gb):\n",
    "        clf = OneVsRestClassifier(base_clf)\n",
    "        cv_results = cross_validate(clf,x_train_tfidf,train_y, cv = 3,n_jobs=-1,return_train_score= True)\n",
    "        print(base_clf)\n",
    "        print('test:', cv_results['test_score'])\n",
    "        print('train:', cv_results['train_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will focus on:\n",
    "\n",
    "* MultinomialNB\n",
    "* RandomForest\n",
    "* LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter for logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "C =uniform(loc=0, scale=4)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "clf = RandomizedSearchCV(lg, hyperparameters, random_state= 42,\n",
    "                         n_iter=100, cv=5, verbose=0, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "best_model = clf.fit(x_train_tfidf,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Penalty: l1\n",
      "Best C: 3.8950220753658367\n"
     ]
    }
   ],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8158079287158121"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparmater for RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [50, 66, 83, 100, 116, 133, 150, 166, 183, 200], 'max_features': ['auto', 'sqrt'], 'min_samples_split': [2, 5, 10]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=50, stop=200, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'min_samples_split': min_samples_split\n",
    "               }\n",
    "\n",
    "print(random_grid)\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                               n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 60 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 60 is smaller than n_iter=100. Running 60 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "rf_random.fit(x_train_tfidf, train_y)\n",
    "rf_random.best_params_\n",
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multinominalNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "\n",
    "\n",
    "# Create regularization hyperparameter distribution using uniform distribution\n",
    "alpha =uniform(loc=0, scale=4)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(alpha = alpha)\n",
    "clf = RandomizedSearchCV(nb, hyperparameters, random_state= 42,\n",
    "                         n_iter=100, cv=5, verbose=0, n_jobs=-1)\n",
    "\n",
    "clf.fit(x_train_tfidf,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameters = {}\n",
    "# grid_search = GridSearchCV(svc, param_grid={}, cv = 3, njob= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find best tfidVector using naivebaye model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', lg),\n",
    "])\n",
    "parameters = {\n",
    "        'vect_ngram_range':(1,2)\n",
    "        'vect__max_df': (0.5, 0.75, 1.0),\n",
    "        'vect_min_df':(1,10),\n",
    "        'vect_max_features':(100,300,500)\n",
    "        'clf': (lg,),\n",
    "        'clf__penalty': ('l1'),\n",
    "        'clf__C': (3.895)}\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv = 3, return_train_score= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
